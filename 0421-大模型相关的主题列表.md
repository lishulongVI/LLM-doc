1. **大模型理论基础**
   - 1.1 起源
   - 1.2 解码注意力机制
   - 1.3 Transformer的崛起
   - 1.4 GPT vs Bert的选择
   - 1.5 GPTs
2. **大模型提示工程**
   - 2.1 提示学习
   - 2.2 思维链
   - 2.3 思维树
   - 2.4 提示工程
   - 2.5 Prompt的构建原则
   - 2.6 Prompt迭代优化方法
   - 2.7 文本总结
3. **Huggingface基础**
   - 3.1 字典与分词工具
   - 3.2 数据集制作
   - 3.3 评价函数
   - 3.4 管道函数
   - 3.5 Trainer
4. **大模型基石**
   - 4.1 Transformer模型总体架构
   - 4.2 注意力机制
   - 4.3 注意力评分函数
   - 4.4 多头注意力
   - 4.5 自注意力
   - 4.6 位置编码
   - 4.7 从零开始实现Transformer的FFN模块
5. **OpenAI嵌入式Embedding模型**
   - 5.1 表示学习与嵌入的概念
   - 5.2 表示学习的原理与算法
   - 5.3 表示学习的评估指标与方法
   - 5.4 Text-embedding系列模型
   - 5.5 Text-embedding-ada系列模型
6. **大模型插件开发实践**
   - 6.1 插件开发概述
   - 6.2 插件应用场景与案例
   - 6.3 待办管理插件
   - 6.4 插件部署与测试
   - 6.5 插件开发实战
7. **OpenAI之多模态开发实战**
   - 7.1 多模态技术概述
   - 7.2 GPT-4v简介与功能特点
   - 7.3 GPT-4v在多模态场景中应用案例
   - 7.4 DALL-E的工作原理与特色
   - 7.5 在文本生成图像领域的应用
   - 7.6 API实战
   - 7.7 TTS技术基础
8. **大模型应用开发框架LangChain**
   - 8.1 模型、提示和解析器
   - 8.2 存储
   - 8.3 模型链
   - 8.4 基于文档的问答
   - 8.5 评估
   - 8.6 Agent
   - 8.7 加载文档
9. **国产大模型ChatGLM深度实战**
   - 9.1 ChatGLM模型概述
   - 9.2 私有化部署
   - 9.3 模型集成与同步
   - 9.4 模型微调
10. **大模型核心硬件选型**
    - 10.1 GPU与显卡概述
    - 10.2 如何根据大模型的需求选择适合的GPU与显卡
    - 10.3 NVIDIA GPU Core与AMD CU
    - 10.4 CUDA Core与Tensor Core
    - 10.5 NVIDIA显卡架构的发展历程
    - 10.6 大模型硬件需求分析
    - 10.7 硬件选型案例分享
11. **从0到1训练私有大模型**
    - 11.1 DPO训练方法的原理与实现
    - 11.2 PPO训练方法的原理与实现
    - 11.3 基于TRL训练大语言模型
    - 11.4 ChatGLM3-6B模型微调实践
12. **大模型微调与部署**
    - 12.1 ChatGLM2微调与ChatGLM3-6B部署微调
    - 12.2 Liama2微调、模型申请、本地部署与结构解析
13. **AutoGPT**
14. **Huggingface实战项目**
    - 14.1 中文分类、中文填空、中文句子关系推断
    - 14.2 预测最后一个词、中间词
    - 14.3 阅读理解
    - 14.4 长序列
15. **国产大模型ChatGLM深度实战**
